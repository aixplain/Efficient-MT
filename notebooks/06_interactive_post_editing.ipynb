{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from vowpalwabbit.DFtoVW import DFtoVW\n",
    "from vowpalwabbit.pyvw import vw\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "''' import AutoVW class from flaml package '''\n",
    "from flaml import AutoVW\n",
    "\n",
    "# Graphical\n",
    "SUPTITLE_FONTSIZE = 20\n",
    "SUPTITLE_FONTWEIGHT = \"bold\"\n",
    "TITLE_FONTSIZE = 15\n",
    "\n",
    "from utils import default_feature_str, default_feature_str, get_test_example, get_vw_examples, get_training_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset in the vowpalwabbit format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =[\"Num token\",\"Num char\",\"Avg word length\",\"Num ADJ\",\"Num ADP\",\"Num ADV\",\"Num AUX\",\"Num CCONJ\",\"Num DET\",\"Num INTJ\",\"Num NOUN\",\"Num NUM\",\"Num PART\",\"Num PRON\",\"Num PROPN\",\"Num PUNCT\",\"Num SCONJ\",\"Num SYM\",\"Num VERB\",\"Num X\",\"Num LOC\",\"Num MISC\",\"Num ORG\",\"Num PER\",\"Num Abbr=Yes\",\"Num Case=Acc\",\"Num Case=Nom\",\"Num Definite=Def\",\"Num Definite=Ind\",\"Num Degree=Cmp\",\"Num Degree=Pos\",\"Num Degree=Sup\",\"Num Foreign=Yes\",\"Num Gender=Fem\",\"Num Gender=Masc\",\"Num Gender=Neut\",\"Num Mood=Imp\",\"Num Mood=Ind\",\"Num NumForm=Digit\",\"Num NumForm=Word\",\"Num NumType=Card\",\"Num NumType=Mult\",\"Num NumType=Ord\",\"Num Number=Plur\",\"Num Number=Sing\",\"Num Person=1\",\"Num Person=2\",\"Num Person=3\",\"Num Polarity=Neg\",\"Num Poss=Yes\",\"Num PronType=Art\",\"Num PronType=Dem\",\"Num PronType=Int\",\"Num PronType=Prs\",\"Num PronType=Rel\",\"Num Reflex=Yes\",\"Num Tense=Past\",\"Num Tense=Pres\",\"Num VerbForm=Fin\",\"Num VerbForm=Ger\",\"Num VerbForm=Inf\",\"Num VerbForm=Part\",\"Num Voice=Pass\",\"Num Style=Expr\",\"Num NumForm=Roman\",\"Num Mood=Cnd\",\"Num Mood=Sub\",\"Num Number[psor]=Plur\",\"Num Number[psor]=Sing\",\"Num Person[psor]=1\",\"Num Person[psor]=2\",\"Num Person[psor]=3\",\"Num PronType=Exc\",\"Num PronType=Ind\",\"Num PronType=Neg\",\"Num Tense=Fut\",\"Num Tense=Imp\",\"Num Typo=Yes\",\"Num Case=Dat\",\"Num Case=Gen\",\"Num Gender[psor]=Masc,Neut\",\"Num Animacy=Anim\",\"Num Animacy=Inan\",\"Num Aspect=Imp\",\"Num Aspect=Perf\",\"Num Case=Ins\",\"Num Case=Loc\",\"Num Variant=Short\",\"Num VerbForm=Conv\",\"Num Voice=Act\",\"Num Voice=Mid\",\"Num AdpType=Comprep\",\"Num AdpType=Prep\",\"Num AdpType=Voc\",\"Num Case=Voc\",\"Num ConjType=Oper\",\"Num Gender=Fem,Masc\",\"Num Gender=Fem,Neut\",\"Num Gender=Masc,Neut\",\"Num Gender[psor]=Fem\",\"Num Gender[psor]=Masc\",\"Num Hyph=Yes\",\"Num NameType=Com\",\"Num NameType=Geo\",\"Num NameType=Giv\",\"Num NameType=Nat\",\"Num NameType=Sur\",\"Num NumType=Frac\",\"Num NumType=Sets\",\"Num NumValue=1\",\"Num NumValue=1,2,3\",\"Num Number=Dual\",\"Num Number=Plur,Sing\",\"Num Polarity=Pos\",\"Num PrepCase=Npr\",\"Num PrepCase=Pre\",\"Num PronType=Emp\",\"Num PronType=Int,Rel\",\"Num PronType=Tot\",\"Num Style=Arch\",\"Num Style=Coll\",\n",
    "        ]\n",
    "        \n",
    "X_train = np.load(\"./data/X_train.npy\")\n",
    "y_train = np.load(\"./data/y_train.npy\")\n",
    "X_test = np.load(\"./data/X_test.npy\")\n",
    "y_test = np.load(\"./data/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "NS_LIST = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n",
    "max_ns_num = 15 # the maximum number of namespaces\n",
    "orginal_dim = 128\n",
    "max_size_per_group = int(np.ceil(orginal_dim / float(max_ns_num)))\n",
    "# sequential grouping\n",
    "group_indexes = []\n",
    "for i in range(max_ns_num):\n",
    "    indexes = [ind for ind in range(i * max_size_per_group,\n",
    "                min((i + 1) * max_size_per_group, orginal_dim))]\n",
    "    if len(indexes) > 0:\n",
    "        group_indexes.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle save and load group_indexes\n",
    "with open(\"./data/group_indexes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(group_indexes, f)\n",
    "with open(\"./data/group_indexes.pkl\", \"rb\") as f:\n",
    "    group_indexes = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_examples = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    ns_content = []\n",
    "    for zz in range(len(group_indexes)):\n",
    "        ns_features = ' '.join('{}:{:.6f}'.format(ind, X_train[i][ind]) for ind in group_indexes[zz])\n",
    "        ns_content.append(ns_features)\n",
    "    ns_line = '{} |{}'.format(str(y_train[i]), '|'.join('{} {}'.format(NS_LIST[j], ns_content[j]) for j in range(len(group_indexes))))\n",
    "    vw_examples.append(ns_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[0]\n",
    "y=y_test[0]\n",
    "\n",
    "def get_training_sample(x, y):\n",
    "    ns_content = []\n",
    "    for zz in range(len(group_indexes)):\n",
    "        ns_features = ' '.join('{}:{:.6f}'.format(ind, x[ind]) for ind in group_indexes[zz])\n",
    "        ns_content.append(ns_features)\n",
    "    ns_line = '{} |{}'.format( str(y), '|'.join('{} {}'.format(NS_LIST[j], ns_content[j]) for j in range(len(group_indexes))))\n",
    "    return ns_line\n",
    "\n",
    "\n",
    "\n",
    "def get_test_sample(x):\n",
    "    ns_content = []\n",
    "    for zz in range(len(group_indexes)):\n",
    "        ns_features = ' '.join('{}:{:.6f}'.format(ind, x[ind]) for ind in group_indexes[zz])\n",
    "        ns_content.append(ns_features)\n",
    "    ns_line = '|{}'.format( '|'.join('{} {}'.format(NS_LIST[j], ns_content[j]) for j in range(len(group_indexes))))\n",
    "    return ns_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vw_examples= get_vw_examples(X_train, y_train, isTrain=False)\n",
    "# vw_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|a 0:8.000000 1:107.000000 2:1.155844 3:-1.000000 4:-1.000000 5:1.000000 6:0.000000 7:0.000000 8:2.000000|b 9:0.000000 10:-1.000000 11:0.000000 12:-1.000000 13:2.000000 14:0.000000 15:3.000000 16:1.000000 17:0.000000|c 18:3.000000 19:0.000000 20:-1.000000 21:2.000000 22:0.000000 23:0.000000 24:0.000000 25:10.000000 26:9.000000|d 27:3.000000 28:-1.000000 29:0.000000 30:-5.000000 31:0.000000 32:0.000000 33:15.000000 34:10.000000 35:5.000000|e 36:0.000000 37:2.000000 38:0.000000 39:0.000000 40:0.000000 41:0.000000 42:0.000000 43:4.000000 44:13.000000|f 45:0.000000 46:0.000000 47:5.000000 48:0.000000 49:0.000000 50:2.000000 51:1.000000 52:0.000000 53:1.000000|g 54:0.000000 55:1.000000 56:-1.000000 57:1.000000 58:2.000000 59:0.000000 60:0.000000 61:1.000000 62:2.000000|h 63:0.000000 64:0.000000 65:0.000000 66:0.000000 67:0.000000 68:1.000000 69:0.000000 70:0.000000 71:0.000000|i 72:0.000000 73:0.000000 74:0.000000 75:0.000000 76:0.000000 77:0.000000 78:8.000000 79:5.000000 80:1.000000|j 81:0.000000 82:0.000000 83:0.000000 84:0.000000 85:0.000000 86:0.000000 87:0.000000 88:0.000000 89:0.000000|k 90:0.000000 91:0.000000 92:0.000000 93:0.000000 94:0.000000 95:0.000000 96:0.000000 97:0.000000 98:0.000000|l 99:0.000000 100:0.000000 101:0.000000 102:0.000000 103:0.000000 104:0.000000 105:0.000000 106:0.000000 107:0.000000|m 108:0.000000 109:0.000000 110:0.000000 111:0.000000 112:0.000000 113:0.000000 114:0.000000 115:0.000000 116:0.000000|n 117:0.000000 118:0.000000 119:0.000000 120:0.000000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_sample(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def online_learning_loop(iter_num, vw_examples, vw_alg, seed=0):\n",
    "    \"\"\"Implements the online learning loop.\n",
    "    \"\"\"\n",
    "    iter_num = len(vw_examples)\n",
    "    print('Online learning for', iter_num, 'steps...')\n",
    "    loss_list = []\n",
    "    for i in range(iter_num):\n",
    "        vw_x = vw_examples[i]\n",
    "        y_true = float(vw_examples[i].split('|')[0])\n",
    "        # predict step\n",
    "        y_pred = vw_alg.predict(vw_x)\n",
    "        # learn step\n",
    "        vw_alg.learn(vw_x)\n",
    "        # calculate one step loss\n",
    "        loss = mean_squared_error([y_pred], [y_true])\n",
    "        loss_list.append(loss)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def query_next_sample(vw_alg, X_pool, n:int=1):\n",
    "    pool_examples = get_vw_examples(X_pool, isTrain=False)\n",
    "\n",
    "    preds = [float(vw_alg.predict(ex)) for ex in pool_examples]\n",
    "    idxs = np.argsort(preds)[::-1]\n",
    "    idxs = idxs[:n]\n",
    "    return idxs\n",
    "\n",
    "import pdb\n",
    "def query_next_sample_interaction(vw_alg, X_pool, n:int=1):\n",
    "    pool_examples = [get_test_sample(_) for _ in X_pool]\n",
    "\n",
    "    preds = [float(vw_alg.predict(ex)) for ex in pool_examples]\n",
    "    idxs = np.argsort(preds)[::-1]\n",
    "    idxs = idxs[:n]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "'''create an AutoVW instance for tuning namespace interactions'''\n",
    "# configure both hyperparamters to tune, e.g., 'interactions', and fixed arguments about the online learner,\n",
    "# e.g., 'quiet' in the search_space argument.\n",
    "autovw_ni = AutoVW(max_live_model_num=5, search_space={'interactions': AutoVW.AUTOMATIC, 'quiet': ''})\n",
    "\n",
    "\n",
    "\n",
    "from vowpalwabbit import pyvw\n",
    "''' create a vanilla vw instance '''\n",
    "vanilla_vw = pyvw.vw('--quiet')\n",
    "\n",
    "vw_alg = autovw_ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying new sample\n",
      "Queried new sample with Label: [2 2 2 2 2 1 3 2 3 1]\n",
      "Predicted [0.0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmi-0a53ca3b3c680e34e/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb#ch0000013vscode-remote?line=28'>29</a>\u001b[0m sample_class_list\u001b[39m.\u001b[39mextend(y_true)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmi-0a53ca3b3c680e34e/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb#ch0000013vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted \u001b[39m\u001b[39m{\u001b[39;00my_pred\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmi-0a53ca3b3c680e34e/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb#ch0000013vscode-remote?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m mean_squared_error([y_pred], [y_true])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmi-0a53ca3b3c680e34e/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb#ch0000013vscode-remote?line=31'>32</a>\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmi-0a53ca3b3c680e34e/home/ahmet/repos/human-benchmark/06_interactive_post_editing.ipynb#ch0000013vscode-remote?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinal progressive validation loss of autovw:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39msum\u001b[39m(loss_list)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(loss_list))\n",
      "File \u001b[0;32m/home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py:438\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=377'>378</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_squared_error\u001b[39m(\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=378'>379</a>\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=379'>380</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=380'>381</a>\u001b[0m     \u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=381'>382</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=382'>383</a>\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=435'>436</a>\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=436'>437</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=437'>438</a>\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=438'>439</a>\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=439'>440</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=440'>441</a>\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=441'>442</a>\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py:105\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=101'>102</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m y_pred\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=104'>105</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=105'>106</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true and y_pred have different number of output (\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m!=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=106'>107</a>\u001b[0m             y_true\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], y_pred\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=107'>108</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=108'>109</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=110'>111</a>\u001b[0m n_outputs \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/krishna/miniconda3/envs/interactive-learning/lib/python3.8/site-packages/sklearn/metrics/_regression.py?line=111'>112</a>\u001b[0m allowed_multioutput_str \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mraw_values\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvariance_weighted\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=10)"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "n = 10\n",
    "X_pool = X_train\n",
    "y_pool = y_train\n",
    "\n",
    "sample_class_list = []\n",
    "i=0\n",
    "while i<100:\n",
    "    X_pool = X_train\n",
    "    y_pool = y_train\n",
    "    print(f\"Querying new sample\")\n",
    "    idx = query_next_sample_interaction(vw_alg, X_pool,n=n)\n",
    "    x = X_pool[idx]\n",
    "    y_true = y_pool[idx]\n",
    "    print(f\"Queried new sample with Label: {y_true}\")\n",
    "    \n",
    "    X_train = np.delete(X_train, idx, axis=0)\n",
    "    y_train = np.delete(y_train, idx, axis=0)\n",
    "    y_pred = []\n",
    "    for x_i, y_i in zip(x, y_true):\n",
    "        # predict step\n",
    "        y_pred_tmp = vw_alg.predict(get_test_sample(x_i))\n",
    "        y_pred.append(y_pred_tmp)\n",
    "        # learn step\n",
    "        # break\n",
    "        vw_alg.learn(get_training_sample(x_i,y_i))\n",
    "        \n",
    "\n",
    "    sample_class_list.extend(y_true)\n",
    "    print(f\"Predicted {y_pred}\")\n",
    "    loss = mean_squared_error([y_pred], [y_true])\n",
    "    loss_list.append(loss)\n",
    "    print('Final progressive validation loss of autovw:', sum(loss_list)/len(loss_list))\n",
    "    plt.figure(i)\n",
    "    sns.histplot(y_test, stat=\"percent\", color=\"red\", label=\"overall_dist\")\n",
    "    sns.histplot(sample_class_list,stat=\"percent\", label=\"online_dist\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./figures_classification_percent_online/{format(i, '05d')}.jpg\")\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = query_next_sample_interaction(vw_alg, X_pool,n=n)\n",
    "vw_alg.learn(get_training_sample(x_i,y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ahmet/repos/data_centric/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 01-25 02:12:15] {2007} INFO - task = classification\n",
      "[flaml.automl: 01-25 02:12:15] {2009} INFO - Data split method: stratified\n",
      "[flaml.automl: 01-25 02:12:15] {2013} INFO - Evaluation method: cv\n",
      "[flaml.automl: 01-25 02:12:15] {1045} INFO - class 0 augmented from 1 to 20\n",
      "[flaml.automl: 01-25 02:12:15] {1045} INFO - class 1 augmented from 6 to 24\n",
      "[flaml.automl: 01-25 02:12:15] {1045} INFO - class 2 augmented from 3 to 21\n",
      "[flaml.automl: 01-25 02:12:15] {2113} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl: 01-25 02:12:15] {2170} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2550} INFO - Estimated sufficient time budget=282s. Estimated necessary time budget=0s.\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.0s,\testimator lgbm's best error=0.7286,\tbest estimator lgbm's best error=0.7286\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.0s,\testimator lgbm's best error=0.7286,\tbest estimator lgbm's best error=0.7286\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.1s,\testimator lgbm's best error=0.3655,\tbest estimator lgbm's best error=0.3655\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.1s,\testimator lgbm's best error=0.0226,\tbest estimator lgbm's best error=0.0226\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.2s,\testimator lgbm's best error=0.0226,\tbest estimator lgbm's best error=0.0226\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.2s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 6, current learner lgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.2s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.3s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.4s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:15] {2597} INFO -  at 0.4s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:15] {2437} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.5s,\testimator lgbm's best error=0.0028,\tbest estimator lgbm's best error=0.0028\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.5s,\testimator lgbm's best error=0.0003,\tbest estimator lgbm's best error=0.0003\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.5s,\testimator lgbm's best error=0.0003,\tbest estimator lgbm's best error=0.0003\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.6s,\testimator lgbm's best error=0.0003,\tbest estimator lgbm's best error=0.0003\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.6s,\testimator lgbm's best error=0.0003,\tbest estimator lgbm's best error=0.0003\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.7s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.7s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.8s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.8s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.8s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 0.9s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.0s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.1s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.1s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.2s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 25, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.2s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.3s,\testimator lgbm's best error=0.0002,\tbest estimator lgbm's best error=0.0002\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 27, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:16] {2597} INFO -  at 1.4s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:16] {2437} INFO - iteration 28, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.4s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 29, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.5s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.5s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.6s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.7s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.7s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.7s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.8s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 36, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.8s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 37, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 1.9s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2437} INFO - iteration 38, current learner lgbm\n",
      "[flaml.automl: 01-25 02:12:17] {2597} INFO -  at 2.0s,\testimator lgbm's best error=0.0001,\tbest estimator lgbm's best error=0.0001\n",
      "[flaml.automl: 01-25 02:12:17] {2815} INFO - retrain lgbm for 0.0s\n",
      "[flaml.automl: 01-25 02:12:17] {2822} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7585626388667795, learning_rate=1.0,\n",
      "               max_bin=127, min_child_samples=2, n_estimators=15, num_leaves=9,\n",
      "               reg_alpha=0.0009765625, reg_lambda=0.020091367474355002,\n",
      "               verbose=-1)\n",
      "[flaml.automl: 01-25 02:12:17] {2199} INFO - fit succeeded\n",
      "[flaml.automl: 01-25 02:12:17] {2200} INFO - Time taken to find the best model: 1.768704891204834\n",
      "[flaml.automl: 01-25 02:12:17] {2211} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "from data_centric.models import ActiveLearner\n",
    "from flaml import AutoML\n",
    "automl_settings = {\n",
    "    \"time_budget\": 2,\n",
    "    \"estimator_list\": ['lgbm'],\n",
    "}\n",
    "\n",
    "# Initialize Learner \n",
    "# TODO: Later we will disable initialization no training samples by providing class names\n",
    "learner = ActiveLearner(\n",
    "    estimator=AutoML(),\n",
    "    embedding_pipeline = \"test embedding pieline\",\n",
    "    X_training=X_train[:10], y_training=y_train[:10], **automl_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "accs = []\n",
    "sample_class_list = []\n",
    "while i<200:\n",
    "    print(f\"Querying new sample\")\n",
    "    # querying for labels\n",
    "    query_idx, query_sample = learner.query(X_train,n_instances=10)\n",
    "    # TODO: ...obtaining new labels from User here\n",
    "    print(f\"Queried new sample with Label: {y_train[query_idx]}\")\n",
    "\n",
    "    y_pred = learner.predict(X_train[query_idx])\n",
    "    print(f\"Predicted {y_pred}\")\n",
    "\n",
    "    # teaching newly labelled examples\n",
    "    learner.teach(\n",
    "        X=X_train[query_idx].reshape(10, -1),\n",
    "        y=y_train[query_idx].reshape(10, ),\n",
    "        **automl_settings\n",
    "    )\n",
    "    sample_class_list.extend(y_train[query_idx])\n",
    "    \n",
    "    X_train = np.delete(X_train, query_idx, axis=0)\n",
    "    y_train = np.delete(y_train, query_idx, axis=0)\n",
    "\n",
    "    # print(f\"acc: {learner.score(X_test, y_test)}\")\n",
    "    # accs.append(learner.score(X_test, y_test))\n",
    "    \n",
    "    plt.figure(i)\n",
    "    sns.histplot(y_test, stat=\"percent\", color=\"red\", label=\"overall_dist\")\n",
    "    sns.histplot(sample_class_list,stat=\"percent\", label=\"online_dist\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./figures_classification_percent_flaml/{format(i, '05d')}.jpg\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b49589068baa6bec98c3349fdcd559e038c68bae5a4c5a7d44d7dd6cd95a33f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('interactive-learning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
